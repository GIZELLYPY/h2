{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados de um banco de dados local Postgres, usandos PySpark e SQL e obtendo alguns insigths\n",
    "\n",
    "By: [Gizélly N.S.](https://www.linkedin.com/in/gizellyns/)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T03:45:29.626016Z",
     "start_time": "2019-12-06T03:45:29.416677Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    FloatType,\n",
    "    DoubleType,\n",
    ")\n",
    "import json\n",
    "\n",
    "from modules.import_conn_properties import import_conn_properties \n",
    "from modules.write_on_db import write_on_db\n",
    "from modules.get_from_db import get_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T03:45:34.343019Z",
     "start_time": "2019-12-06T03:45:29.628615Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"01_sql\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.10.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lendo arquivo em formato CSV e carregando em Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clients_from_csv(path: str):\n",
    "    \"\"\" Returns \"clientes\" dataframe from csv file.\n",
    "\n",
    "    Args:\n",
    "        path (str): where is located clientes.csv\n",
    "\n",
    "    Returns:\n",
    "        df: spark dataframe  with  \"clientes\" data\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    client_schema = StructType(\n",
    "        [\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"sexo\", StringType(), True),\n",
    "            StructField(\"data_nascimento\", DateType(), True),\n",
    "            StructField(\"data_cadastro\", TimestampType(), True),\n",
    "            StructField(\"cidade\", StringType(), True),\n",
    "            StructField(\"sigla\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .load(path, schema=client_schema)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df_clients = read_clients_from_csv('./files/clientes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "|    id|sexo|data_nascimento|      data_cadastro|        cidade|sigla|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "| 94722|   m|     1982-04-17|2023-03-17 12:41:30|     São Paulo|   SP|\n",
      "|   505|   m|     1985-03-29|2017-10-11 22:21:33|     São Paulo|   SP|\n",
      "| 43346|   m|     1991-06-11|2020-12-16 13:08:00|         Belém|   PA|\n",
      "| 34349|   m|     1991-10-08|2020-03-26 01:47:56|      Capivari|   SP|\n",
      "|100525|   m|     1992-07-29|2023-03-27 00:19:30|     São Paulo|   SP|\n",
      "| 97089|   m|     1993-05-24|2023-03-21 13:36:54|     São Paulo|   SP|\n",
      "| 40554|null|     1984-03-12|2020-07-31 03:30:26|     São Paulo|   SP|\n",
      "| 84489|   m|     2004-06-17|2023-02-13 22:32:06|     São Paulo|   SP|\n",
      "| 94679|   m|     1999-04-06|2023-03-17 11:14:28|     São Paulo|   SP|\n",
      "|  4381|   m|     1996-09-16|2018-05-11 18:01:44|     São Paulo|   SP|\n",
      "| 99490|   m|     1985-02-04|2023-03-25 15:15:34|     São Paulo|   SP|\n",
      "| 74136|null|     1986-10-04|2022-12-01 12:24:51|     São Paulo|   SP|\n",
      "| 92872|   m|     1988-11-19|2023-03-14 05:44:39|     São Paulo|   SP|\n",
      "|110430|   m|     2002-04-20|2023-04-12 22:16:07|     São Paulo|   SP|\n",
      "| 94013|   m|     1994-08-13|2023-03-16 01:47:33|     São Paulo|   SP|\n",
      "|102924|   m|     2004-03-16|2023-03-30 15:52:33|     São Paulo|   SP|\n",
      "| 58115|   f|     2000-05-08|2022-04-06 20:32:25|Belo Horizonte|   MG|\n",
      "|102457|   m|     1992-07-01|2023-03-29 20:29:52|     São Paulo|   SP|\n",
      "| 54833|   m|     1993-02-15|2022-01-25 18:17:00|     São Paulo|   SP|\n",
      "| 97533|   m|     1994-10-20|2023-03-22 03:46:31|     São Paulo|   SP|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_from_csv(path:str):\n",
    "    \"\"\" Returns \"resultado\" dataframe from csv file.\n",
    "\n",
    "    Args:\n",
    "        path (str): where is located resultado.csv\n",
    "\n",
    "    Returns:\n",
    "        df: spark dataframe  with  \"resultado\" data\n",
    "    \"\"\"    \n",
    "    client_schema = StructType(\n",
    "        [\n",
    "            StructField(\"data_acesso\", TimestampType(), True),\n",
    "            StructField(\"clientes_id\", IntegerType(), True),\n",
    "            StructField(\"buyin\", FloatType(), True),\n",
    "            StructField(\"rake\", DoubleType(), True),\n",
    "            StructField(\"winning\", DoubleType(), True)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"encoding\", \"UTF-8\")\n",
    "        .load(path, schema=client_schema)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_results = read_results_from_csv('./files/resultado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-----+-------------------+-------------------+\n",
      "|        data_acesso|clientes_id|buyin|               rake|            winning|\n",
      "+-------------------+-----------+-----+-------------------+-------------------+\n",
      "|2022-08-14 00:00:00|        505| 25.0|               1.75|              -2.75|\n",
      "|2022-08-15 00:00:00|        505| 20.0|                4.0|  8.399999618530273|\n",
      "|2022-08-16 00:00:00|        505| 65.0|  5.400000035762787|-19.300000309944153|\n",
      "|2022-08-17 00:00:00|        505| 30.0| 2.6500000953674316|              -30.0|\n",
      "|2022-08-19 00:00:00|        505|  5.0| 0.3499999940395355|  3.549999952316284|\n",
      "|2022-08-20 00:00:00|        505| 15.0|  1.899999976158142|               0.25|\n",
      "|2022-08-23 00:00:00|        505| 10.0|  2.950000047683716|-3.0999999046325684|\n",
      "|2022-08-30 00:00:00|        505|  5.0|0.30000001192092896|               -5.0|\n",
      "|2022-11-20 00:00:00|        505| 43.0|    7.3999999538064|              -33.0|\n",
      "|2022-11-21 00:00:00|        505| 15.0|  1.550000011920929| 16.400000393390656|\n",
      "|2022-11-23 00:00:00|        505| 69.0| 5.8499999195337296|-29.549999594688416|\n",
      "|2022-12-04 00:00:00|        505| 26.0|  4.550000071525574|-7.1499998569488525|\n",
      "|2022-12-07 00:00:00|        505| 31.0|               2.25|-16.399999916553497|\n",
      "|2022-12-08 00:00:00|        505| 23.0|  4.200000077486038|-12.299999952316284|\n",
      "|2022-12-14 00:00:00|        505| 36.0|  5.050000011920929|  32.74999809265137|\n",
      "|2022-12-15 00:00:00|        505| 39.0| 3.3500000834465027| 100.70000267028809|\n",
      "|2022-12-16 00:00:00|        505| 53.0|  5.800000011920929| -39.30000019073486|\n",
      "|2022-12-20 00:00:00|        505| 78.0| 10.200000025331974| -60.64999994635582|\n",
      "|2022-12-21 00:00:00|        505| 11.0|  1.100000038743019| 12.100000381469727|\n",
      "|2022-12-22 00:00:00|        505| 21.0|  2.100000023841858|              -21.0|\n",
      "+-------------------+-----------+-----+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Tabelas no Postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T03:45:40.268164Z",
     "start_time": "2019-12-06T03:45:39.954014Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela clients criada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "%run -i './sql/create_table_clientes.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela results criada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "%run -i './sql/create_table_results.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo no banco de dados: Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_properties = import_conn_properties('./conn_properties/clients_properties.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sucess'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_on_db(spark, df_clients, clients_properties) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escrevendo no banco de dados: Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_properties = import_conn_properties('./conn_properties/results_properties.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sucess'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_on_db(spark, df_results, results_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo os dados inseridos no banco de dados para fazer as análises, guardando o retorno em tabelas temporárias em tempo de execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "|    id|sexo|data_nascimento|      data_cadastro|        cidade|sigla|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "| 94722|   m|     1982-04-17|2023-03-17 12:41:30|     São Paulo|   SP|\n",
      "|   505|   m|     1985-03-29|2017-10-11 22:21:33|     São Paulo|   SP|\n",
      "| 43346|   m|     1991-06-11|2020-12-16 13:08:00|         Belém|   PA|\n",
      "| 34349|   m|     1991-10-08|2020-03-26 01:47:56|      Capivari|   SP|\n",
      "|100525|   m|     1992-07-29|2023-03-27 00:19:30|     São Paulo|   SP|\n",
      "| 97089|   m|     1993-05-24|2023-03-21 13:36:54|     São Paulo|   SP|\n",
      "| 40554|null|     1984-03-12|2020-07-31 03:30:26|     São Paulo|   SP|\n",
      "| 84489|   m|     2004-06-17|2023-02-13 22:32:06|     São Paulo|   SP|\n",
      "| 94679|   m|     1999-04-06|2023-03-17 11:14:28|     São Paulo|   SP|\n",
      "|  4381|   m|     1996-09-16|2018-05-11 18:01:44|     São Paulo|   SP|\n",
      "| 99490|   m|     1985-02-04|2023-03-25 15:15:34|     São Paulo|   SP|\n",
      "| 74136|null|     1986-10-04|2022-12-01 12:24:51|     São Paulo|   SP|\n",
      "| 92872|   m|     1988-11-19|2023-03-14 05:44:39|     São Paulo|   SP|\n",
      "|110430|   m|     2002-04-20|2023-04-12 22:16:07|     São Paulo|   SP|\n",
      "| 94013|   m|     1994-08-13|2023-03-16 01:47:33|     São Paulo|   SP|\n",
      "|102924|   m|     2004-03-16|2023-03-30 15:52:33|     São Paulo|   SP|\n",
      "| 58115|   f|     2000-05-08|2022-04-06 20:32:25|Belo Horizonte|   MG|\n",
      "|102457|   m|     1992-07-01|2023-03-29 20:29:52|     São Paulo|   SP|\n",
      "| 54833|   m|     1993-02-15|2022-01-25 18:17:00|     São Paulo|   SP|\n",
      "| 97533|   m|     1994-10-20|2023-03-22 03:46:31|     São Paulo|   SP|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_from_db(spark, clients_properties, \"clientes\")\n",
    "\n",
    "spark.sql(\"SELECT  * FROM clientes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "|    id|sexo|data_nascimento|      data_cadastro|        cidade|sigla|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "| 94722|   m|     1982-04-17|2023-03-17 12:41:30|     São Paulo|   SP|\n",
      "|   505|   m|     1985-03-29|2017-10-11 22:21:33|     São Paulo|   SP|\n",
      "| 43346|   m|     1991-06-11|2020-12-16 13:08:00|         Belém|   PA|\n",
      "| 34349|   m|     1991-10-08|2020-03-26 01:47:56|      Capivari|   SP|\n",
      "|100525|   m|     1992-07-29|2023-03-27 00:19:30|     São Paulo|   SP|\n",
      "| 97089|   m|     1993-05-24|2023-03-21 13:36:54|     São Paulo|   SP|\n",
      "| 40554|null|     1984-03-12|2020-07-31 03:30:26|     São Paulo|   SP|\n",
      "| 84489|   m|     2004-06-17|2023-02-13 22:32:06|     São Paulo|   SP|\n",
      "| 94679|   m|     1999-04-06|2023-03-17 11:14:28|     São Paulo|   SP|\n",
      "|  4381|   m|     1996-09-16|2018-05-11 18:01:44|     São Paulo|   SP|\n",
      "| 99490|   m|     1985-02-04|2023-03-25 15:15:34|     São Paulo|   SP|\n",
      "| 74136|null|     1986-10-04|2022-12-01 12:24:51|     São Paulo|   SP|\n",
      "| 92872|   m|     1988-11-19|2023-03-14 05:44:39|     São Paulo|   SP|\n",
      "|110430|   m|     2002-04-20|2023-04-12 22:16:07|     São Paulo|   SP|\n",
      "| 94013|   m|     1994-08-13|2023-03-16 01:47:33|     São Paulo|   SP|\n",
      "|102924|   m|     2004-03-16|2023-03-30 15:52:33|     São Paulo|   SP|\n",
      "| 58115|   f|     2000-05-08|2022-04-06 20:32:25|Belo Horizonte|   MG|\n",
      "|102457|   m|     1992-07-01|2023-03-29 20:29:52|     São Paulo|   SP|\n",
      "| 54833|   m|     1993-02-15|2022-01-25 18:17:00|     São Paulo|   SP|\n",
      "| 97533|   m|     1994-10-20|2023-03-22 03:46:31|     São Paulo|   SP|\n",
      "+------+----+---------------+-------------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_from_db(spark, results_properties, \"resultado\")\n",
    "\n",
    "spark.sql(\"SELECT  * FROM clientes\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respondendo a perguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-06T03:45:40.600010Z",
     "start_time": "2019-12-06T03:45:40.278813Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "### Quanto de rake foi gerado por cada Geração de jogadores?\n",
    "\n",
    "Cada geração tendo o seguinte critério:\n",
    "\n",
    "* Veteranos, geração formada por pessoas que nasceram entre 1925 e 1940.\n",
    "* Baby Boomers são os nascidos entre 1941 e 1959.\n",
    "* Geração X, que compreende o período de 1960 a 1979.\n",
    "* Geração Y é composta por indivíduos que nasceram entre 1980 e 1995.\n",
    "* Geração Z é composta com os nascidos a partir de 1996 até 2010.\n",
    "* Geração Alpha, engloba todos os nascidos a partir de 2010 até a presente data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|baby_boomers_total_rake|geracao_x_total_rake|geracao_y_total_rake|geracao_z_total_rake|geracao_alpha_total_rake|\n",
      "+-----------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|   38549.66000577434...|520650.7700801343...|612412.5099798794...|93378.69005352071...|                    null|\n",
      "+-----------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rake_by_generation = spark.sql(\"SELECT  \\\n",
    "                                   SUM(r.rake) FILTER (WHERE c.data_nascimento BETWEEN '1941-01-01' AND '1959-12-31') AS baby_boomers, \\\n",
    "                                   SUM(r.rake) FILTER (WHERE c.data_nascimento BETWEEN '1960-01-01' AND '1979-12-31') AS geracao_x, \\\n",
    "                                   SUM(r.rake) FILTER (WHERE c.data_nascimento BETWEEN '1980-01-01' AND '1995-12-31') AS geracao_y, \\\n",
    "                                   SUM(r.rake) FILTER (WHERE c.data_nascimento BETWEEN '1996-01-01' AND '2009-12-31') AS geracao_z, \\\n",
    "                                   SUM(r.rake) FILTER (WHERE c.data_nascimento >= '2010-01-01') as geracao_alpha \\\n",
    "                               FROM \\\n",
    "                                    clientes c \\\n",
    "                                INNER JOIN resultado r ON c.id = r.clientes_id GROUP BY r.rake\\\n",
    "                                \")\n",
    "\n",
    "\n",
    "rake_by_generation.select(F.sum(rake_by_generation.baby_boomers).alias(\"baby_boomers_total_rake\"),\n",
    "                          F.sum(rake_by_generation.geracao_x).alias(\"geracao_x_total_rake\"),\n",
    "                          F.sum(rake_by_generation.geracao_y).alias(\"geracao_y_total_rake\"),\n",
    "                          F.sum(rake_by_generation.geracao_z).alias(\"geracao_z_total_rake\"),\n",
    "                          F.sum(rake_by_generation.geracao_alpha).alias(\"geracao_alpha_total_rake\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observação: Não há dados sobre veteranos, geração formada por pessoas que nasceram entre 1925 e 1940 e o total rake para  Geração Alpha é inexistente na base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual foi o rake gerado por mês?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "|mes|year|          total_rake|\n",
      "+---+----+--------------------+\n",
      "|  6|2020|9349.120016444473...|\n",
      "|  7|2020|12373.77001768350...|\n",
      "|  8|2020|18130.99000354483...|\n",
      "|  9|2020|11097.09996528737...|\n",
      "| 10|2020|7260.410003677009...|\n",
      "| 11|2020|7699.459994543344...|\n",
      "| 12|2020|7708.860011853279...|\n",
      "|  1|2021|10875.24000201374...|\n",
      "|  2|2021|15381.23000405542...|\n",
      "|  3|2021|28560.31008905358...|\n",
      "|  4|2021|63925.37990119870...|\n",
      "|  5|2021|10196.66995991766...|\n",
      "|  6|2021|7312.099979553371...|\n",
      "|  7|2021|8615.850006807594...|\n",
      "|  8|2021|7137.780008828268...|\n",
      "|  9|2021|15338.07001414522...|\n",
      "| 10|2021|24094.74004009738...|\n",
      "| 11|2021|26819.62997406721...|\n",
      "| 12|2021|36064.35995101556...|\n",
      "|  1|2022|40423.61998029797...|\n",
      "+---+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_acesso = spark.sql(\"SELECT data_acesso \\\n",
    "FROM resultado \\\n",
    "GROUP BY data_acesso \\\n",
    "ORDER BY data_acesso DESC \\\n",
    "\")\n",
    "dt_acesso.createOrReplaceTempView(\"data_acesso\")\n",
    "\n",
    "rake_por_mes = spark.sql(\"SELECT EXTRACT(MONTH FROM data_acesso) AS mes, \\\n",
    "EXTRACT(YEAR FROM data_acesso) AS year, \\\n",
    "SUM(rake) AS total_rake \\\n",
    "FROM resultado \\\n",
    "GROUP BY EXTRACT(MONTH FROM data_acesso), EXTRACT(YEAR FROM data_acesso) \\\n",
    "ORDER BY year, mes \\\n",
    "\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qual sexo tem uma maior proporção de ganhadores?\n",
    "Como ganhador, considere um jogador com Winning maior que 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------------------------+\n",
      "|proporcao_winning_feminino (%)|proporcao_winning_masculino (%)|\n",
      "+------------------------------+-------------------------------+\n",
      "|                      7.098200|                      84.433700|\n",
      "+------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winning_genero = spark.sql(\"SELECT SUM(r.winning) FILTER (WHERE r.winning > 0)AS total_winning, \\\n",
    "                            SUM(r.winning) FILTER (WHERE c.sexo = 'm' AND r.winning > 0) AS total_masculino, \\\n",
    "                            SUM(r.winning)  FILTER (WHERE c.sexo = 'f' AND r.winning > 0) AS total_feminino \\\n",
    "                            FROM \\\n",
    "                                clientes c \\\n",
    "                            INNER JOIN resultado r ON c.id = r.clientes_id GROUP BY r.winning\\\n",
    "                            \")\n",
    "\n",
    "winning_genero.select((F.sum(winning_genero.total_feminino)/F.sum(winning_genero.total_winning)*100).alias(\"proporcao_winning_feminino (%)\"),\n",
    "                     (F.sum(winning_genero.total_masculino)/F.sum(winning_genero.total_winning)*100).alias(\"proporcao_winning_masculino (%)\")).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
